meta:
  device: 'cuda'
  seed: 123
  out_root_dir: 'out_llama/'
  use_neptune: true
server:
  model:
    cfg_path: 'configs/model/llama7b.yaml'
    skip: false
    response_max_len: 1600 # important if generating! 
  watermark:  # Disabled
    scheme: 'kgw'
    generation:
      - seeding_scheme: ['selfhash'] 
        gamma: 0.25
        delta: 4.0
    detection:
      normalizers: []
      ignore_repeated_ngrams: true 
      z_threshold: 4.0 
  disable_watermark: True
attacker:
  algo: 'our'
  model:
    cfg_path: 'configs/model/mistral7b.yaml'
    skip: true
    use_sampling: true
  querying:
    skip: true
    dataset: 'c4'
    batch_size: 64
    start_from_batch_num: 0
    end_at_batch_num: 500
    apply_watermark: true 
  learning:
    skip: true
    mode: "fast"
    nb_queries: 30000
  generation:
    sysprompt: 'standard'
    spoofer_strength: 3
    w_abcd: 0.0
    w_partials: 0.0
    w_empty: 1.0
    w_future: 0.0
    min_wm_count_nonempty: 2 
    min_wm_mass_empty: 0.00007
    future_num_cands: 5
    future_num_options_per_fillin: 10
    future_prefix_size: 10 
    future_local_w_decay: 0.9 
    panic_from: 750
    repetition_penalty: 1.0
    use_ft_counts: true
    prevent_eos_if_zest_bad: true
    use_graceful_conclusion: true 
evaluator:
  skip: false
  batch_size: 4
  eval_class: "server"
  metrics:
    - "detector"
  eval_mode:
    - "dolly-writing-100-long"
gradio:
  skip: true 
  make_public: false 
  port: 7860
  default_prompt: "Write a long essay about war."
